Russell Meng xm21

(1) Run the program BenchmarkForAutocomplete and copy/paste the 
results here this for #matches = 20

search	size	#match	binary	brute
	456976	20	0.1120	0.0107
a	17576	20	0.0028	0.0081
b	17576	20	0.0030	0.0063
c	17576	20	0.0030	0.0034
x	17576	20	0.0028	0.0033
y	17576	20	0.0028	0.0034
z	17576	20	0.0040	0.0035
aa	676		20	0.0001	0.0032
az	676		20	0.0001	0.0032
za	676		20	0.0001	0.0032
zz	676		20	0.0001	0.0037



(2) Run the program again for #matches = 10000, paste the results, 
and then make any conclusions about how the # matches 
effects the runtime. 

#matches does not significantly influence the runtime

search	size	#match	binary	brute
	456976	10000	0.1102	0.0282
a	17576	10000	0.0028	0.0100
b	17576	10000	0.0030	0.0166
c	17576	10000	0.0027	0.0066
x	17576	10000	0.0028	0.0089
y	17576	10000	0.0028	0.0064
z	17576	10000	0.0026	0.0062
aa	676	    10000	0.0001	0.0049
az	676	    10000	0.0001	0.0038
za	676	    10000	0.0001	0.0035
zz	676		10000	0.0001	0.0034

(3) Copy/paste the code from BruteAutocomplete.topMatches below. 
Explain what the Big-Oh complexity of the entire loop: 
for(Term t : myTerms) {...} 
is in terms of N, the number of elements in myTerms and 
M, the number of terms that match the prefix. 
Assume that every priority-queue operation runs in O(log k) time. 
Explain your answer which should be in terms of N, M, and k.

for the operations inside the loop: for each t that matches the prefix, we do the operations inside the loop, so the big O for these operations in total is O(M log k), if we do nothing for a mismatch;
for the loop: O(N);
so in total it is O(N-M+MlogK).

public List<Term> topMatches(String prefix, int k) {
		if (k < 0) {
			throw new IllegalArgumentException("Illegal value of k:"+k);
		}
		
		// maintain pq of size k
		PriorityQueue<Term> pq = new PriorityQueue<Term>(10, new Term.WeightOrder());
		for (Term t : myTerms) {
			if (!t.getWord().startsWith(prefix))
				continue;
			if (pq.size() < k) {
				pq.add(t);
			} else if (pq.peek().getWeight() < t.getWeight()) {
				pq.remove();
				pq.add(t);
			}
		}
		int numResults = Math.min(k, pq.size());
		LinkedList<Term> ret = new LinkedList<>();
		for (int i = 0; i < numResults; i++) {
			ret.addFirst(pq.remove());
		}
		return ret;
	}


(4) Explain why the last for loop in BruteAutocomplete.topMatches 
uses a LinkedList (and not an ArrayList) 
AND why the PriorityQueue uses Term.WeightOrder to get 
the top k heaviest matches -- rather than 
using Term.ReverseWeightOrder.

the time complexity to add a first element for a linked list is O(1), but O(n) for an ArrayList;
because for the BruteAutocomplete.topMatches, it uses addFirst which puts those words in correct order; but if it were an ArrayList and had used .add(), we would put the weightiest one to the end of the list, so in that case we need reverse order

(5) Explain what the runtime of the 
BinarySearchAutocomplete.topMatches code that you 
implemented is by copy/pasting the code below 
and explaining your answer in terms of N, M, and k.

firstIndex and lastIndex: O(logN)
sort the list: O(MlogM)
add the first k matches: O(k)
so in total: O(logN+MlogM+k)

public List<Term> topMatches(String prefix, int k) {
		if (prefix == null) throw new NullPointerException();
		ArrayList<Term> list = new ArrayList<>();
		if (k == 0) return list;
		int first = firstIndexOf(myTerms,new Term(prefix, 0.0), new Term.PrefixOrder(prefix.length()));
		int last = lastIndexOf(myTerms,new Term(prefix, 0.0), new Term.PrefixOrder(prefix.length()));
		if (first == -1) return list;
		for (int i = first; i <= last; i++){
			list.add(myTerms[i]);
		}
		Comparator<Term> reverseComp = new Term.ReverseWeightOrder();
		Collections.sort(list, reverseComp);
		ArrayList<Term> klist = new ArrayList<Term>();
		if(k>=list.size()) return list;
		for (int j = 0; j < k; j++){
			klist.add(list.get(j));
		}
		return klist;
	}

